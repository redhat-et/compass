{
  "models": [
    {
      "model_id": "meta-llama/llama-3.1-8b-instruct",
      "name": "Llama 3.1 8B Instruct",
      "provider": "Meta",
      "family": "llama3",
      "size_parameters": "8B",
      "context_length": 128000,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "question_answering",
        "summarization"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Llama 3.1 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 16,
      "recommended_for": [
        "chatbot_conversational",
        "code_generation_detailed",
        "translation",
        "summarization_short",
        "content_generation"
      ],
      "approval_status": "approved",
      "notes": "Excellent general-purpose model with strong instruction following"
    },
    {
      "model_id": "meta-llama/llama-3.1-70b-instruct",
      "name": "Llama 3.1 70B Instruct",
      "provider": "Meta",
      "family": "llama3",
      "size_parameters": "70B",
      "context_length": 128000,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "question_answering",
        "summarization",
        "reasoning"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Llama 3.1 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 80,
      "recommended_for": [
        "chatbot_conversational",
        "code_generation_detailed",
        "content_generation",
        "summarization_short",
        "document_analysis_rag",
        "long_document_summarization",
        "research_legal_analysis"
      ],
      "approval_status": "approved",
      "notes": "High-performance model for complex reasoning tasks"
    },
    {
      "model_id": "meta-llama/llama-3.3-70b-instruct",
      "name": "Llama 3.3 70B Instruct",
      "provider": "Meta",
      "family": "llama3",
      "size_parameters": "70B",
      "context_length": 128000,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "question_answering",
        "summarization",
        "reasoning"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Llama 3.3 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 80,
      "recommended_for": [
        "code_generation_detailed",
        "content_generation",
        "summarization_short",
        "document_analysis_rag",
        "long_document_summarization",
        "research_legal_analysis"
      ],
      "approval_status": "approved",
      "notes": "High-performance model for complex reasoning tasks"
    },
    {
      "model_id": "meta-llama/llama-4-scout-17b-16e-instruct",
      "name": "Llama 4 Scout 17B Instruct",
      "provider": "Meta",
      "family": "llama4",
      "size_parameters": "17B",
      "context_length": 128000,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "reasoning"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Llama 4 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 34,
      "recommended_for": [
        "chatbot_conversational",
        "content_generation",
        "summarization_short",
        "code_generation_detailed"
      ],
      "approval_status": "approved",
      "notes": "Next-generation Llama model with improved reasoning"
    },
    {
      "model_id": "meta-llama/llama-4-maverick-17b-128e-instruct",
      "name": "Llama 4 Maverick 17B Instruct",
      "provider": "Meta",
      "family": "llama4",
      "size_parameters": "17B",
      "context_length": 128000,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "reasoning"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Llama 4 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 34,
      "recommended_for": [
        "chatbot_conversational",
        "content_generation",
        "code_generation_detailed"
      ],
      "approval_status": "approved",
      "notes": "Llama 4 variant optimized for instruction following"
    },
    {
      "model_id": "meta-llama/llama-4-maverick-17b-128e-instruct-fp8",
      "name": "Llama 4 Maverick 17B Instruct FP8",
      "provider": "Meta",
      "family": "llama4",
      "size_parameters": "17B",
      "context_length": 128000,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "reasoning"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Llama 4 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 17,
      "recommended_for": [
        "chatbot_conversational",
        "content_generation",
        "code_generation_detailed"
      ],
      "approval_status": "approved",
      "notes": "FP8 quantized variant for efficient inference"
    },
    {
      "model_id": "mistralai/mistral-7b-instruct-v0.3",
      "name": "Mistral 7B Instruct v0.3",
      "provider": "Mistral AI",
      "family": "mistral",
      "size_parameters": "7B",
      "context_length": 32768,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation"
      ],
      "domain_specialization": [
        "general",
        "code"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 14,
      "recommended_for": [
        "chatbot_conversational",
        "code_generation_detailed",
        "translation",
        "content_generation"
      ],
      "approval_status": "approved",
      "notes": "Efficient 7B Mistral model with strong code capabilities"
    },
    {
      "model_id": "mistralai/mistral-small-24b-instruct-2501",
      "name": "Mistral Small 24B Instruct",
      "provider": "Mistral AI",
      "family": "mistral",
      "size_parameters": "24B",
      "context_length": 32768,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation"
      ],
      "domain_specialization": [
        "general",
        "code"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 48,
      "recommended_for": [
        "chatbot_conversational",
        "code_generation_detailed",
        "translation",
        "content_generation"
      ],
      "approval_status": "approved",
      "notes": "Efficient model with strong code capabilities"
    },
    {
      "model_id": "mistralai/mistral-small-3.1-24b-instruct-2503",
      "name": "Mistral Small 3.1 24B Instruct",
      "provider": "Mistral AI",
      "family": "mistral",
      "size_parameters": "24B",
      "context_length": 32768,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation"
      ],
      "domain_specialization": [
        "general",
        "code"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 48,
      "recommended_for": [
        "chatbot_conversational",
        "code_generation_detailed",
        "translation",
        "content_generation"
      ],
      "approval_status": "approved",
      "notes": "Latest Mistral Small with improved performance"
    },
    {
      "model_id": "mistralai/mixtral-8x7b-instruct-v0.1",
      "name": "Mixtral 8x7B Instruct",
      "provider": "Mistral AI",
      "family": "mixtral",
      "size_parameters": "8x7B",
      "context_length": 32768,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation",
        "multilingual"
      ],
      "domain_specialization": [
        "general",
        "code",
        "multilingual"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 80,
      "recommended_for": [
        "code_generation_detailed",
        "content_generation",
        "translation"
      ],
      "approval_status": "approved",
      "notes": "Mixture-of-experts model with excellent multilingual support"
    },
    {
      "model_id": "mistralai/mixtral-8x22b-instruct-v0.1",
      "name": "Mixtral 8x22B Instruct",
      "provider": "Mistral AI",
      "family": "mixtral",
      "size_parameters": "8x22B",
      "context_length": 65536,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation",
        "multilingual",
        "reasoning"
      ],
      "domain_specialization": [
        "general",
        "code",
        "multilingual"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 176,
      "recommended_for": [
        "code_generation_detailed",
        "content_generation",
        "translation",
        "document_analysis_rag",
        "research_legal_analysis"
      ],
      "approval_status": "approved",
      "notes": "Large mixture-of-experts model with excellent reasoning and multilingual support"
    },
    {
      "model_id": "ibm-granite/granite-3.0-8b-instruct",
      "name": "Granite 3.0 8B Instruct",
      "provider": "IBM",
      "family": "granite",
      "size_parameters": "8B",
      "context_length": 128000,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "summarization",
        "rag"
      ],
      "domain_specialization": [
        "general",
        "enterprise"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 16,
      "recommended_for": [
        "chatbot_conversational",
        "document_analysis_rag",
        "summarization_short",
        "long_document_summarization"
      ],
      "approval_status": "approved",
      "notes": "Enterprise-focused IBM Granite model with strong RAG capabilities"
    },
    {
      "model_id": "ibm-granite/granite-3.1-8b-instruct",
      "name": "Granite 3.1 8B Instruct",
      "provider": "IBM",
      "family": "granite",
      "size_parameters": "8B",
      "context_length": 128000,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "summarization",
        "rag"
      ],
      "domain_specialization": [
        "general",
        "enterprise"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 16,
      "recommended_for": [
        "chatbot_conversational",
        "document_analysis_rag",
        "summarization_short",
        "long_document_summarization"
      ],
      "approval_status": "approved",
      "notes": "Enterprise-focused model with strong RAG capabilities"
    },
    {
      "model_id": "qwen/qwen2.5-7b-instruct",
      "name": "Qwen 2.5 7B Instruct",
      "provider": "Alibaba Cloud",
      "family": "qwen",
      "size_parameters": "7B",
      "context_length": 131072,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation",
        "multilingual"
      ],
      "domain_specialization": [
        "general",
        "code",
        "multilingual"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 14,
      "recommended_for": [
        "chatbot_conversational",
        "code_generation_detailed",
        "translation"
      ],
      "approval_status": "approved",
      "notes": "Strong multilingual and code capabilities with large context window"
    },
    {
      "model_id": "qwen/qwen3-8b-fp8",
      "name": "Qwen3 8B FP8",
      "provider": "Alibaba Cloud",
      "family": "qwen",
      "size_parameters": "8B",
      "context_length": 131072,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation",
        "multilingual"
      ],
      "domain_specialization": [
        "general",
        "code",
        "multilingual"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 8,
      "recommended_for": [
        "chatbot_conversational",
        "code_generation_detailed",
        "translation"
      ],
      "approval_status": "approved",
      "notes": "FP8 quantized model for efficient inference with large context"
    },
    {
      "model_id": "qwen/qwen2.5-72b-instruct",
      "name": "Qwen 2.5 72B Instruct",
      "provider": "Alibaba Cloud",
      "family": "qwen",
      "size_parameters": "72B",
      "context_length": 131072,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation",
        "multilingual",
        "reasoning"
      ],
      "domain_specialization": [
        "general",
        "code",
        "multilingual"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 144,
      "recommended_for": [
        "chatbot_conversational",
        "code_generation_detailed",
        "translation",
        "document_analysis_rag",
        "research_legal_analysis"
      ],
      "approval_status": "approved",
      "notes": "Large Qwen model with strong reasoning and multilingual capabilities"
    },
    {
      "model_id": "microsoft/phi-4",
      "name": "Phi-4",
      "provider": "Microsoft",
      "family": "phi",
      "size_parameters": "14B",
      "context_length": 16384,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "reasoning",
        "code_generation"
      ],
      "domain_specialization": [
        "general",
        "code"
      ],
      "license": "MIT",
      "license_type": "permissive",
      "min_gpu_memory_gb": 28,
      "recommended_for": [
        "code_completion",
        "code_generation_detailed",
        "chatbot_conversational"
      ],
      "approval_status": "approved",
      "notes": "Compact model with strong reasoning and code capabilities"
    },
    {
      "model_id": "nvidia/llama-3.1-nemotron-70b-instruct-hf",
      "name": "Llama 3.1 Nemotron 70B Instruct",
      "provider": "NVIDIA",
      "family": "llama3-nemotron",
      "size_parameters": "70B",
      "context_length": 128000,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "reasoning",
        "summarization"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "NVIDIA Open Model License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 80,
      "recommended_for": [
        "document_analysis_rag",
        "long_document_summarization",
        "research_legal_analysis",
        "content_generation"
      ],
      "approval_status": "approved",
      "notes": "NVIDIA-optimized Llama variant with enhanced instruction following"
    },
    {
      "model_id": "openai/gpt-oss-20b",
      "name": "GPT-OSS 20B",
      "provider": "OpenAI",
      "family": "gpt",
      "size_parameters": "20B",
      "context_length": 8192,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 40,
      "recommended_for": [
        "chatbot_conversational",
        "code_generation_detailed",
        "content_generation"
      ],
      "approval_status": "approved",
      "notes": "Open-source GPT-style model"
    },
    {
      "model_id": "openai/gpt-oss-120b",
      "name": "GPT-OSS 120B",
      "provider": "OpenAI",
      "family": "gpt",
      "size_parameters": "120B",
      "context_length": 8192,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation",
        "reasoning"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 240,
      "recommended_for": [
        "content_generation",
        "code_generation_detailed",
        "document_analysis_rag"
      ],
      "approval_status": "approved",
      "notes": "Large open-source GPT model"
    },
    {
      "model_id": "redhatai/meta-llama-3.1-8b-instruct-fp8-dynamic",
      "name": "Llama 3.1 8B Instruct FP8",
      "provider": "Red Hat",
      "family": "llama3",
      "size_parameters": "8B",
      "context_length": 128000,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "question_answering"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Llama 3.1 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 8,
      "recommended_for": [
        "chatbot_conversational",
        "code_generation_detailed",
        "translation"
      ],
      "approval_status": "approved",
      "notes": "FP8 quantized Llama 3.1 for efficient inference"
    },
    {
      "model_id": "redhatai/llama-3.3-70b-instruct-fp8-dynamic",
      "name": "Llama 3.3 70B Instruct FP8",
      "provider": "Red Hat",
      "family": "llama3",
      "size_parameters": "70B",
      "context_length": 128000,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "reasoning"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Llama 3.3 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 40,
      "recommended_for": [
        "document_analysis_rag",
        "long_document_summarization",
        "research_legal_analysis"
      ],
      "approval_status": "approved",
      "notes": "FP8 quantized Llama 3.3 for cost-effective deployment"
    },
    {
      "model_id": "redhatai/llama-3.3-70b-instruct-quantized.w4a16",
      "name": "Llama 3.3 70B Instruct W4A16",
      "provider": "Red Hat",
      "family": "llama3",
      "size_parameters": "70B",
      "context_length": 128000,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "reasoning"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Llama 3.3 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 25,
      "recommended_for": [
        "document_analysis_rag",
        "long_document_summarization"
      ],
      "approval_status": "approved",
      "notes": "4-bit weight quantized for very efficient inference"
    },
    {
      "model_id": "redhatai/llama-3.3-70b-instruct-quantized.w8a8",
      "name": "Llama 3.3 70B Instruct W8A8",
      "provider": "Red Hat",
      "family": "llama3",
      "size_parameters": "70B",
      "context_length": 128000,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "reasoning"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Llama 3.3 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 35,
      "recommended_for": [
        "document_analysis_rag",
        "long_document_summarization"
      ],
      "approval_status": "approved",
      "notes": "8-bit quantized for balanced quality and efficiency"
    },
    {
      "model_id": "redhatai/llama-4-scout-17b-16e-instruct-fp8-dynamic",
      "name": "Llama 4 Scout 17B Instruct FP8",
      "provider": "Red Hat",
      "family": "llama4",
      "size_parameters": "17B",
      "context_length": 128000,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "reasoning"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Llama 4 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 17,
      "recommended_for": [
        "chatbot_conversational",
        "content_generation",
        "code_generation_detailed"
      ],
      "approval_status": "approved",
      "notes": "FP8 quantized Llama 4 Scout"
    },
    {
      "model_id": "redhatai/llama-4-scout-17b-16e-instruct-quantized.w4a16",
      "name": "Llama 4 Scout 17B Instruct W4A16",
      "provider": "Red Hat",
      "family": "llama4",
      "size_parameters": "17B",
      "context_length": 128000,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "reasoning"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Llama 4 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 10,
      "recommended_for": [
        "chatbot_conversational",
        "content_generation"
      ],
      "approval_status": "approved",
      "notes": "4-bit quantized for maximum efficiency"
    },
    {
      "model_id": "redhatai/llama-3.1-nemotron-70b-instruct-hf-fp8-dynamic",
      "name": "Llama 3.1 Nemotron 70B Instruct FP8",
      "provider": "Red Hat",
      "family": "llama3-nemotron",
      "size_parameters": "70B",
      "context_length": 128000,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "reasoning"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "NVIDIA Open Model License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 40,
      "recommended_for": [
        "document_analysis_rag",
        "long_document_summarization",
        "research_legal_analysis"
      ],
      "approval_status": "approved",
      "notes": "FP8 quantized Nemotron variant"
    },
    {
      "model_id": "redhatai/mistral-small-3.1-24b-instruct-2503-fp8-dynamic",
      "name": "Mistral Small 3.1 24B Instruct FP8",
      "provider": "Red Hat",
      "family": "mistral",
      "size_parameters": "24B",
      "context_length": 32768,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation"
      ],
      "domain_specialization": [
        "general",
        "code"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 24,
      "recommended_for": [
        "chatbot_conversational",
        "code_generation_detailed",
        "translation"
      ],
      "approval_status": "approved",
      "notes": "FP8 quantized Mistral Small for efficient code generation"
    },
    {
      "model_id": "redhatai/mistral-small-3.1-24b-instruct-2503-quantized.w4a16",
      "name": "Mistral Small 3.1 24B Instruct W4A16",
      "provider": "Red Hat",
      "family": "mistral",
      "size_parameters": "24B",
      "context_length": 32768,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation"
      ],
      "domain_specialization": [
        "general",
        "code"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 15,
      "recommended_for": [
        "chatbot_conversational",
        "code_generation_detailed"
      ],
      "approval_status": "approved",
      "notes": "4-bit quantized for efficient deployment"
    },
    {
      "model_id": "redhatai/mistral-small-3.1-24b-instruct-2503-quantized.w8a8",
      "name": "Mistral Small 3.1 24B Instruct W8A8",
      "provider": "Red Hat",
      "family": "mistral",
      "size_parameters": "24B",
      "context_length": 32768,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation"
      ],
      "domain_specialization": [
        "general",
        "code"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 20,
      "recommended_for": [
        "chatbot_conversational",
        "code_generation_detailed"
      ],
      "approval_status": "approved",
      "notes": "8-bit quantized for balanced performance"
    },
    {
      "model_id": "redhatai/granite-3.1-8b-instruct-fp8-dynamic",
      "name": "Granite 3.1 8B Instruct FP8",
      "provider": "Red Hat",
      "family": "granite",
      "size_parameters": "8B",
      "context_length": 128000,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "summarization"
      ],
      "domain_specialization": [
        "general",
        "enterprise"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 8,
      "recommended_for": [
        "chatbot_conversational",
        "document_analysis_rag",
        "summarization_short"
      ],
      "approval_status": "approved",
      "notes": "FP8 quantized Granite for efficient enterprise deployment"
    },
    {
      "model_id": "redhatai/granite-3.1-8b-instruct-quantized.w4a16",
      "name": "Granite 3.1 8B Instruct W4A16",
      "provider": "Red Hat",
      "family": "granite",
      "size_parameters": "8B",
      "context_length": 128000,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "summarization"
      ],
      "domain_specialization": [
        "general",
        "enterprise"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 5,
      "recommended_for": [
        "chatbot_conversational",
        "summarization_short"
      ],
      "approval_status": "approved",
      "notes": "4-bit quantized Granite for maximum efficiency"
    },
    {
      "model_id": "redhatai/phi-4-fp8-dynamic",
      "name": "Phi-4 FP8",
      "provider": "Red Hat",
      "family": "phi",
      "size_parameters": "14B",
      "context_length": 16384,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation"
      ],
      "domain_specialization": [
        "general",
        "code"
      ],
      "license": "MIT",
      "license_type": "permissive",
      "min_gpu_memory_gb": 14,
      "recommended_for": [
        "code_completion",
        "code_generation_detailed",
        "chatbot_conversational"
      ],
      "approval_status": "approved",
      "notes": "FP8 quantized Phi-4"
    },
    {
      "model_id": "redhatai/phi-4-quantized.w4a16",
      "name": "Phi-4 W4A16",
      "provider": "Red Hat",
      "family": "phi",
      "size_parameters": "14B",
      "context_length": 16384,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation"
      ],
      "domain_specialization": [
        "general",
        "code"
      ],
      "license": "MIT",
      "license_type": "permissive",
      "min_gpu_memory_gb": 8,
      "recommended_for": [
        "code_completion",
        "code_generation_detailed"
      ],
      "approval_status": "approved",
      "notes": "4-bit quantized Phi-4"
    },
    {
      "model_id": "redhatai/phi-4-quantized.w8a8",
      "name": "Phi-4 W8A8",
      "provider": "Red Hat",
      "family": "phi",
      "size_parameters": "14B",
      "context_length": 16384,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation"
      ],
      "domain_specialization": [
        "general",
        "code"
      ],
      "license": "MIT",
      "license_type": "permissive",
      "min_gpu_memory_gb": 10,
      "recommended_for": [
        "code_completion",
        "code_generation_detailed"
      ],
      "approval_status": "approved",
      "notes": "8-bit quantized Phi-4"
    },
    {
      "model_id": "redhatai/qwen2.5-7b-instruct-fp8-dynamic",
      "name": "Qwen 2.5 7B Instruct FP8",
      "provider": "Red Hat",
      "family": "qwen",
      "size_parameters": "7B",
      "context_length": 131072,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation",
        "multilingual"
      ],
      "domain_specialization": [
        "general",
        "code",
        "multilingual"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 7,
      "recommended_for": [
        "chatbot_conversational",
        "code_generation_detailed",
        "translation"
      ],
      "approval_status": "approved",
      "notes": "FP8 quantized Qwen 2.5"
    },
    {
      "model_id": "redhatai/qwen2.5-7b-instruct-quantized.w4a16",
      "name": "Qwen 2.5 7B Instruct W4A16",
      "provider": "Red Hat",
      "family": "qwen",
      "size_parameters": "7B",
      "context_length": 131072,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation",
        "multilingual"
      ],
      "domain_specialization": [
        "general",
        "code",
        "multilingual"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 4,
      "recommended_for": [
        "chatbot_conversational",
        "code_generation_detailed"
      ],
      "approval_status": "approved",
      "notes": "4-bit quantized Qwen 2.5"
    },
    {
      "model_id": "redhatai/qwen2.5-7b-instruct-quantized.w8a8",
      "name": "Qwen 2.5 7B Instruct W8A8",
      "provider": "Red Hat",
      "family": "qwen",
      "size_parameters": "7B",
      "context_length": 131072,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation",
        "multilingual"
      ],
      "domain_specialization": [
        "general",
        "code",
        "multilingual"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 5,
      "recommended_for": [
        "chatbot_conversational",
        "code_generation_detailed"
      ],
      "approval_status": "approved",
      "notes": "8-bit quantized Qwen 2.5"
    },
    {
      "model_id": "redhatai/qwen3-8b-fp8-dynamic",
      "name": "Qwen3 8B FP8 Dynamic",
      "provider": "Red Hat",
      "family": "qwen",
      "size_parameters": "8B",
      "context_length": 131072,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "code_generation",
        "multilingual"
      ],
      "domain_specialization": [
        "general",
        "code",
        "multilingual"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 8,
      "recommended_for": [
        "chatbot_conversational",
        "code_generation_detailed",
        "translation"
      ],
      "approval_status": "approved",
      "notes": "FP8 dynamic quantized Qwen3"
    },
    {
      "model_id": "redhatai/smollm3-3b-fp8-dynamic",
      "name": "SmolLM3 3B FP8",
      "provider": "Red Hat",
      "family": "smollm",
      "size_parameters": "3B",
      "context_length": 8192,
      "supported_tasks": [
        "chat",
        "instruction_following"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 3,
      "recommended_for": [
        "chatbot_conversational",
        "code_completion"
      ],
      "approval_status": "approved",
      "notes": "Ultra-compact model for edge deployment"
    },
    {
      "model_id": "redhatai/nvidia-nemotron-nano-9b-v2-fp8-dynamic",
      "name": "NVIDIA Nemotron Nano 9B FP8",
      "provider": "Red Hat",
      "family": "nemotron",
      "size_parameters": "9B",
      "context_length": 8192,
      "supported_tasks": [
        "chat",
        "instruction_following"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "NVIDIA Open Model License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 9,
      "recommended_for": [
        "chatbot_conversational",
        "content_generation"
      ],
      "approval_status": "approved",
      "notes": "Compact NVIDIA Nemotron variant"
    },
    {
      "model_id": "google/gemma-2-9b-it",
      "name": "Gemma 2 9B Instruct",
      "provider": "Google",
      "family": "gemma",
      "size_parameters": "9B",
      "context_length": 8192,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "question_answering"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Gemma License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 18,
      "recommended_for": [
        "chatbot_conversational",
        "content_generation",
        "summarization_short"
      ],
      "approval_status": "approved",
      "notes": "Google's efficient Gemma 2 model with strong instruction following"
    },
    {
      "model_id": "google/gemma-2-27b-it",
      "name": "Gemma 2 27B Instruct",
      "provider": "Google",
      "family": "gemma",
      "size_parameters": "27B",
      "context_length": 8192,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "question_answering",
        "reasoning"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Gemma License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 54,
      "recommended_for": [
        "chatbot_conversational",
        "content_generation",
        "summarization_short",
        "document_analysis_rag"
      ],
      "approval_status": "approved",
      "notes": "Large Gemma 2 model with improved reasoning capabilities"
    },
    {
      "model_id": "redhatai/gemma-3n-e4b-it-fp8-dynamic",
      "name": "Gemma 3N E4B FP8",
      "provider": "Red Hat",
      "family": "gemma",
      "size_parameters": "4B",
      "context_length": 8192,
      "supported_tasks": [
        "chat",
        "instruction_following"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Gemma License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 4,
      "recommended_for": [
        "chatbot_conversational"
      ],
      "approval_status": "approved",
      "notes": "FP8 quantized Gemma variant"
    },
    {
      "model_id": "redhatai/deepseek-r1-0528-quantized.w4a16",
      "name": "DeepSeek R1 W4A16",
      "provider": "Red Hat",
      "family": "deepseek",
      "size_parameters": "7B",
      "context_length": 16384,
      "supported_tasks": [
        "chat",
        "instruction_following",
        "reasoning"
      ],
      "domain_specialization": [
        "general"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 4,
      "recommended_for": [
        "chatbot_conversational",
        "code_generation_detailed"
      ],
      "approval_status": "approved",
      "notes": "4-bit quantized DeepSeek reasoning model"
    },
    {
      "model_id": "redhatai/kimi-k2-instruct-quantized.w4a16",
      "name": "Kimi K2 Instruct W4A16",
      "provider": "Red Hat",
      "family": "kimi",
      "size_parameters": "7B",
      "context_length": 16384,
      "supported_tasks": [
        "chat",
        "instruction_following"
      ],
      "domain_specialization": [
        "general",
        "multilingual"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 4,
      "recommended_for": [
        "chatbot_conversational",
        "translation"
      ],
      "approval_status": "approved",
      "notes": "4-bit quantized Kimi model with multilingual support"
    },
    {
      "model_id": "redhatai/qwen3-coder-480b-a35b-instruct-fp8",
      "name": "Qwen3 Coder 480B FP8",
      "provider": "Red Hat",
      "family": "qwen",
      "size_parameters": "480B",
      "context_length": 32768,
      "supported_tasks": [
        "code_generation",
        "instruction_following"
      ],
      "domain_specialization": [
        "code"
      ],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 240,
      "recommended_for": [
        "code_generation_detailed"
      ],
      "approval_status": "approved",
      "notes": "Massive code-specialized model"
    }
  ],
  "gpu_types": [
    {
      "gpu_type": "L4",
      "aliases": ["NVIDIA-L4", "L4"],
      "memory_gb": 24,
      "compute_capability": "8.9",
      "typical_use_cases": ["inference"],
      "cost_per_hour_usd": 0.65,
      "cost_per_hour_aws": 0.80,
      "cost_per_hour_gcp": 0.70,
      "cost_per_hour_azure": 0.85,
      "availability": "high",
      "notes": "Entry-level inference GPU, cost-effective for small models"
    },
    {
      "gpu_type": "A10G",
      "aliases": ["NVIDIA-A10G", "A10G"],
      "memory_gb": 24,
      "compute_capability": "8.6",
      "typical_use_cases": ["inference"],
      "cost_per_hour_usd": 1.00,
      "cost_per_hour_aws": 1.20,
      "cost_per_hour_gcp": 1.10,
      "cost_per_hour_azure": 1.25,
      "availability": "high",
      "notes": "Good balance of cost and performance for medium models"
    },
    {
      "gpu_type": "A100-40",
      "aliases": ["NVIDIA-A100-40GB", "A100-40", "A100-40GB"],
      "memory_gb": 40,
      "compute_capability": "8.0",
      "typical_use_cases": ["inference", "training"],
      "cost_per_hour_usd": 1.50,
      "cost_per_hour_aws": 3.06,
      "cost_per_hour_gcp": 2.93,
      "cost_per_hour_azure": 3.40,
      "availability": "medium",
      "notes": "Production inference, market minimum ~$1.50/hr (Artificial Analysis)"
    },
    {
      "gpu_type": "A100-80",
      "aliases": ["NVIDIA-A100-80GB", "A100-80", "A100-80GB"],
      "memory_gb": 80,
      "compute_capability": "8.0",
      "typical_use_cases": ["inference", "training"],
      "cost_per_hour_usd": 2.00,
      "cost_per_hour_aws": 4.10,
      "cost_per_hour_gcp": 3.67,
      "cost_per_hour_azure": 4.00,
      "availability": "medium",
      "notes": "Large model inference, 2x memory of A100-40"
    },
    {
      "gpu_type": "H100",
      "aliases": ["NVIDIA-H100", "H100", "H100-80GB"],
      "memory_gb": 80,
      "compute_capability": "9.0",
      "typical_use_cases": ["inference", "training"],
      "cost_per_hour_usd": 2.70,
      "cost_per_hour_aws": 4.90,
      "cost_per_hour_gcp": 4.23,
      "cost_per_hour_azure": 5.50,
      "availability": "medium",
      "notes": "Flagship inference GPU, market minimum ~$2.70/hr (Artificial Analysis)"
    },
    {
      "gpu_type": "H200",
      "aliases": ["NVIDIA-H200", "H200", "H200-141GB"],
      "memory_gb": 141,
      "compute_capability": "9.0",
      "typical_use_cases": ["inference", "training"],
      "cost_per_hour_usd": 3.50,
      "cost_per_hour_aws": 6.50,
      "cost_per_hour_gcp": 5.80,
      "cost_per_hour_azure": 7.00,
      "availability": "low",
      "notes": "Extended memory H100, ideal for 70B+ models"
    },
    {
      "gpu_type": "B200",
      "aliases": ["NVIDIA-B200", "B200"],
      "memory_gb": 192,
      "compute_capability": "10.0",
      "typical_use_cases": ["inference", "training"],
      "cost_per_hour_usd": 5.50,
      "cost_per_hour_aws": 8.50,
      "cost_per_hour_gcp": 7.50,
      "cost_per_hour_azure": 9.00,
      "availability": "very_low",
      "notes": "Next-gen Blackwell architecture, market minimum ~$5.50/hr"
    },
    {
      "gpu_type": "MI300X",
      "aliases": ["AMD-MI300X", "MI300X", "AMD-Instinct-MI300X"],
      "memory_gb": 192,
      "compute_capability": "N/A",
      "typical_use_cases": ["inference", "training"],
      "cost_per_hour_usd": 2.00,
      "cost_per_hour_aws": null,
      "cost_per_hour_gcp": null,
      "cost_per_hour_azure": null,
      "availability": "low",
      "notes": "AMD competitor to H100, good value at ~$2.00/hr"
    }
  ]
}