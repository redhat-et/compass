{
  "models": [
    {
      "model_id": "meta-llama/Llama-3.1-8B-Instruct",
      "name": "Llama 3.1 8B Instruct",
      "provider": "Meta",
      "family": "llama3",
      "size_parameters": "8B",
      "context_length": 128000,
      "supported_tasks": ["chat", "instruction_following", "question_answering", "summarization"],
      "domain_specialization": ["general"],
      "license": "Llama 3.1 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 16,
      "recommended_for": ["chatbot_conversational", "code_generation_detailed", "translation", "summarization_short", "content_generation"],
      "approval_status": "approved",
      "notes": "Excellent general-purpose model with strong instruction following"
    },
    {
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "name": "Llama 3.3 70B Instruct",
      "provider": "Meta",
      "family": "llama3",
      "size_parameters": "70B",
      "context_length": 128000,
      "supported_tasks": ["chat", "instruction_following", "question_answering", "summarization", "reasoning"],
      "domain_specialization": ["general"],
      "license": "Llama 3.3 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 80,
      "recommended_for": ["code_generation_detailed", "content_generation", "summarization_short", "document_analysis_rag", "long_document_summarization", "research_legal_analysis"],
      "approval_status": "approved",
      "notes": "High-performance model for complex reasoning tasks"
    },
    {
      "model_id": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
      "name": "Llama 4 Scout 17B Instruct",
      "provider": "Meta",
      "family": "llama4",
      "size_parameters": "17B",
      "context_length": 128000,
      "supported_tasks": ["chat", "instruction_following", "reasoning"],
      "domain_specialization": ["general"],
      "license": "Llama 4 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 34,
      "recommended_for": ["chatbot_conversational", "content_generation", "summarization_short", "code_generation_detailed"],
      "approval_status": "approved",
      "notes": "Next-generation Llama model with improved reasoning"
    },
    {
      "model_id": "meta-llama/Llama-4-Maverick-17B-128E-Instruct",
      "name": "Llama 4 Maverick 17B Instruct",
      "provider": "Meta",
      "family": "llama4",
      "size_parameters": "17B",
      "context_length": 128000,
      "supported_tasks": ["chat", "instruction_following", "reasoning"],
      "domain_specialization": ["general"],
      "license": "Llama 4 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 34,
      "recommended_for": ["chatbot_conversational", "content_generation", "code_generation_detailed"],
      "approval_status": "approved",
      "notes": "Llama 4 variant optimized for instruction following"
    },
    {
      "model_id": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
      "name": "Llama 4 Maverick 17B Instruct FP8",
      "provider": "Meta",
      "family": "llama4",
      "size_parameters": "17B",
      "context_length": 128000,
      "supported_tasks": ["chat", "instruction_following", "reasoning"],
      "domain_specialization": ["general"],
      "license": "Llama 4 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 17,
      "recommended_for": ["chatbot_conversational", "content_generation", "code_generation_detailed"],
      "approval_status": "approved",
      "notes": "FP8 quantized variant for efficient inference"
    },
    {
      "model_id": "mistralai/Mistral-Small-24B-Instruct-2501",
      "name": "Mistral Small 24B Instruct",
      "provider": "Mistral AI",
      "family": "mistral",
      "size_parameters": "24B",
      "context_length": 32768,
      "supported_tasks": ["chat", "instruction_following", "code_generation"],
      "domain_specialization": ["general", "code"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 48,
      "recommended_for": ["chatbot_conversational", "code_generation_detailed", "translation", "content_generation"],
      "approval_status": "approved",
      "notes": "Efficient model with strong code capabilities"
    },
    {
      "model_id": "mistralai/Mistral-Small-3.1-24B-Instruct-2503",
      "name": "Mistral Small 3.1 24B Instruct",
      "provider": "Mistral AI",
      "family": "mistral",
      "size_parameters": "24B",
      "context_length": 32768,
      "supported_tasks": ["chat", "instruction_following", "code_generation"],
      "domain_specialization": ["general", "code"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 48,
      "recommended_for": ["chatbot_conversational", "code_generation_detailed", "translation", "content_generation"],
      "approval_status": "approved",
      "notes": "Latest Mistral Small with improved performance"
    },
    {
      "model_id": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "name": "Mixtral 8x7B Instruct",
      "provider": "Mistral AI",
      "family": "mixtral",
      "size_parameters": "8x7B",
      "context_length": 32768,
      "supported_tasks": ["chat", "instruction_following", "code_generation", "multilingual"],
      "domain_specialization": ["general", "code", "multilingual"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 80,
      "recommended_for": ["code_generation_detailed", "content_generation", "translation"],
      "approval_status": "approved",
      "notes": "Mixture-of-experts model with excellent multilingual support"
    },
    {
      "model_id": "ibm-granite/granite-3.1-8b-instruct",
      "name": "Granite 3.1 8B Instruct",
      "provider": "IBM",
      "family": "granite",
      "size_parameters": "8B",
      "context_length": 128000,
      "supported_tasks": ["chat", "instruction_following", "summarization", "rag"],
      "domain_specialization": ["general", "enterprise"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 16,
      "recommended_for": ["chatbot_conversational", "document_analysis_rag", "summarization_short", "long_document_summarization"],
      "approval_status": "approved",
      "notes": "Enterprise-focused model with strong RAG capabilities"
    },
    {
      "model_id": "Qwen/Qwen2.5-7B-Instruct",
      "name": "Qwen 2.5 7B Instruct",
      "provider": "Alibaba Cloud",
      "family": "qwen",
      "size_parameters": "7B",
      "context_length": 131072,
      "supported_tasks": ["chat", "instruction_following", "code_generation", "multilingual"],
      "domain_specialization": ["general", "code", "multilingual"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 14,
      "recommended_for": ["chatbot_conversational", "code_generation_detailed", "translation"],
      "approval_status": "approved",
      "notes": "Strong multilingual and code capabilities with large context window"
    },
    {
      "model_id": "Qwen/Qwen3-8B-FP8",
      "name": "Qwen3 8B FP8",
      "provider": "Alibaba Cloud",
      "family": "qwen",
      "size_parameters": "8B",
      "context_length": 131072,
      "supported_tasks": ["chat", "instruction_following", "code_generation", "multilingual"],
      "domain_specialization": ["general", "code", "multilingual"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 8,
      "recommended_for": ["chatbot_conversational", "code_generation_detailed", "translation"],
      "approval_status": "approved",
      "notes": "FP8 quantized model for efficient inference with large context"
    },
    {
      "model_id": "microsoft/phi-4",
      "name": "Phi-4",
      "provider": "Microsoft",
      "family": "phi",
      "size_parameters": "14B",
      "context_length": 16384,
      "supported_tasks": ["chat", "instruction_following", "reasoning", "code_generation"],
      "domain_specialization": ["general", "code"],
      "license": "MIT",
      "license_type": "permissive",
      "min_gpu_memory_gb": 28,
      "recommended_for": ["code_completion", "code_generation_detailed", "chatbot_conversational"],
      "approval_status": "approved",
      "notes": "Compact model with strong reasoning and code capabilities"
    },
    {
      "model_id": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF",
      "name": "Llama 3.1 Nemotron 70B Instruct",
      "provider": "NVIDIA",
      "family": "llama3-nemotron",
      "size_parameters": "70B",
      "context_length": 128000,
      "supported_tasks": ["chat", "instruction_following", "reasoning", "summarization"],
      "domain_specialization": ["general"],
      "license": "NVIDIA Open Model License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 80,
      "recommended_for": ["document_analysis_rag", "long_document_summarization", "research_legal_analysis", "content_generation"],
      "approval_status": "approved",
      "notes": "NVIDIA-optimized Llama variant with enhanced instruction following"
    },
    {
      "model_id": "openai/gpt-oss-20b",
      "name": "GPT-OSS 20B",
      "provider": "OpenAI",
      "family": "gpt",
      "size_parameters": "20B",
      "context_length": 8192,
      "supported_tasks": ["chat", "instruction_following", "code_generation"],
      "domain_specialization": ["general"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 40,
      "recommended_for": ["chatbot_conversational", "code_generation_detailed", "content_generation"],
      "approval_status": "approved",
      "notes": "Open-source GPT-style model"
    },
    {
      "model_id": "openai/gpt-oss-120b",
      "name": "GPT-OSS 120B",
      "provider": "OpenAI",
      "family": "gpt",
      "size_parameters": "120B",
      "context_length": 8192,
      "supported_tasks": ["chat", "instruction_following", "code_generation", "reasoning"],
      "domain_specialization": ["general"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 240,
      "recommended_for": ["content_generation", "code_generation_detailed", "document_analysis_rag"],
      "approval_status": "approved",
      "notes": "Large open-source GPT model"
    },
    {
      "model_id": "RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic",
      "name": "Llama 3.1 8B Instruct FP8",
      "provider": "Red Hat",
      "family": "llama3",
      "size_parameters": "8B",
      "context_length": 128000,
      "supported_tasks": ["chat", "instruction_following", "question_answering"],
      "domain_specialization": ["general"],
      "license": "Llama 3.1 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 8,
      "recommended_for": ["chatbot_conversational", "code_generation_detailed", "translation"],
      "approval_status": "approved",
      "notes": "FP8 quantized Llama 3.1 for efficient inference"
    },
    {
      "model_id": "RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic",
      "name": "Llama 3.3 70B Instruct FP8",
      "provider": "Red Hat",
      "family": "llama3",
      "size_parameters": "70B",
      "context_length": 128000,
      "supported_tasks": ["chat", "instruction_following", "reasoning"],
      "domain_specialization": ["general"],
      "license": "Llama 3.3 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 40,
      "recommended_for": ["document_analysis_rag", "long_document_summarization", "research_legal_analysis"],
      "approval_status": "approved",
      "notes": "FP8 quantized Llama 3.3 for cost-effective deployment"
    },
    {
      "model_id": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16",
      "name": "Llama 3.3 70B Instruct W4A16",
      "provider": "Red Hat",
      "family": "llama3",
      "size_parameters": "70B",
      "context_length": 128000,
      "supported_tasks": ["chat", "instruction_following", "reasoning"],
      "domain_specialization": ["general"],
      "license": "Llama 3.3 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 25,
      "recommended_for": ["document_analysis_rag", "long_document_summarization"],
      "approval_status": "approved",
      "notes": "4-bit weight quantized for very efficient inference"
    },
    {
      "model_id": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w8a8",
      "name": "Llama 3.3 70B Instruct W8A8",
      "provider": "Red Hat",
      "family": "llama3",
      "size_parameters": "70B",
      "context_length": 128000,
      "supported_tasks": ["chat", "instruction_following", "reasoning"],
      "domain_specialization": ["general"],
      "license": "Llama 3.3 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 35,
      "recommended_for": ["document_analysis_rag", "long_document_summarization"],
      "approval_status": "approved",
      "notes": "8-bit quantized for balanced quality and efficiency"
    },
    {
      "model_id": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic",
      "name": "Llama 4 Scout 17B Instruct FP8",
      "provider": "Red Hat",
      "family": "llama4",
      "size_parameters": "17B",
      "context_length": 128000,
      "supported_tasks": ["chat", "instruction_following", "reasoning"],
      "domain_specialization": ["general"],
      "license": "Llama 4 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 17,
      "recommended_for": ["chatbot_conversational", "content_generation", "code_generation_detailed"],
      "approval_status": "approved",
      "notes": "FP8 quantized Llama 4 Scout"
    },
    {
      "model_id": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-quantized.w4a16",
      "name": "Llama 4 Scout 17B Instruct W4A16",
      "provider": "Red Hat",
      "family": "llama4",
      "size_parameters": "17B",
      "context_length": 128000,
      "supported_tasks": ["chat", "instruction_following", "reasoning"],
      "domain_specialization": ["general"],
      "license": "Llama 4 Community License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 10,
      "recommended_for": ["chatbot_conversational", "content_generation"],
      "approval_status": "approved",
      "notes": "4-bit quantized for maximum efficiency"
    },
    {
      "model_id": "RedHatAI/Llama-3.1-Nemotron-70B-Instruct-HF-FP8-dynamic",
      "name": "Llama 3.1 Nemotron 70B Instruct FP8",
      "provider": "Red Hat",
      "family": "llama3-nemotron",
      "size_parameters": "70B",
      "context_length": 128000,
      "supported_tasks": ["chat", "instruction_following", "reasoning"],
      "domain_specialization": ["general"],
      "license": "NVIDIA Open Model License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 40,
      "recommended_for": ["document_analysis_rag", "long_document_summarization", "research_legal_analysis"],
      "approval_status": "approved",
      "notes": "FP8 quantized Nemotron variant"
    },
    {
      "model_id": "RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-FP8-dynamic",
      "name": "Mistral Small 3.1 24B Instruct FP8",
      "provider": "Red Hat",
      "family": "mistral",
      "size_parameters": "24B",
      "context_length": 32768,
      "supported_tasks": ["chat", "instruction_following", "code_generation"],
      "domain_specialization": ["general", "code"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 24,
      "recommended_for": ["chatbot_conversational", "code_generation_detailed", "translation"],
      "approval_status": "approved",
      "notes": "FP8 quantized Mistral Small for efficient code generation"
    },
    {
      "model_id": "RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w4a16",
      "name": "Mistral Small 3.1 24B Instruct W4A16",
      "provider": "Red Hat",
      "family": "mistral",
      "size_parameters": "24B",
      "context_length": 32768,
      "supported_tasks": ["chat", "instruction_following", "code_generation"],
      "domain_specialization": ["general", "code"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 15,
      "recommended_for": ["chatbot_conversational", "code_generation_detailed"],
      "approval_status": "approved",
      "notes": "4-bit quantized for efficient deployment"
    },
    {
      "model_id": "RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-quantized.w8a8",
      "name": "Mistral Small 3.1 24B Instruct W8A8",
      "provider": "Red Hat",
      "family": "mistral",
      "size_parameters": "24B",
      "context_length": 32768,
      "supported_tasks": ["chat", "instruction_following", "code_generation"],
      "domain_specialization": ["general", "code"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 20,
      "recommended_for": ["chatbot_conversational", "code_generation_detailed"],
      "approval_status": "approved",
      "notes": "8-bit quantized for balanced performance"
    },
    {
      "model_id": "RedHatAI/granite-3.1-8b-instruct-fp8-dynamic",
      "name": "Granite 3.1 8B Instruct FP8",
      "provider": "Red Hat",
      "family": "granite",
      "size_parameters": "8B",
      "context_length": 128000,
      "supported_tasks": ["chat", "instruction_following", "summarization"],
      "domain_specialization": ["general", "enterprise"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 8,
      "recommended_for": ["chatbot_conversational", "document_analysis_rag", "summarization_short"],
      "approval_status": "approved",
      "notes": "FP8 quantized Granite for efficient enterprise deployment"
    },
    {
      "model_id": "RedHatAI/granite-3.1-8b-instruct-quantized.w4a16",
      "name": "Granite 3.1 8B Instruct W4A16",
      "provider": "Red Hat",
      "family": "granite",
      "size_parameters": "8B",
      "context_length": 128000,
      "supported_tasks": ["chat", "instruction_following", "summarization"],
      "domain_specialization": ["general", "enterprise"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 5,
      "recommended_for": ["chatbot_conversational", "summarization_short"],
      "approval_status": "approved",
      "notes": "4-bit quantized Granite for maximum efficiency"
    },
    {
      "model_id": "RedHatAI/phi-4-FP8-dynamic",
      "name": "Phi-4 FP8",
      "provider": "Red Hat",
      "family": "phi",
      "size_parameters": "14B",
      "context_length": 16384,
      "supported_tasks": ["chat", "instruction_following", "code_generation"],
      "domain_specialization": ["general", "code"],
      "license": "MIT",
      "license_type": "permissive",
      "min_gpu_memory_gb": 14,
      "recommended_for": ["code_completion", "code_generation_detailed", "chatbot_conversational"],
      "approval_status": "approved",
      "notes": "FP8 quantized Phi-4"
    },
    {
      "model_id": "RedHatAI/phi-4-quantized.w4a16",
      "name": "Phi-4 W4A16",
      "provider": "Red Hat",
      "family": "phi",
      "size_parameters": "14B",
      "context_length": 16384,
      "supported_tasks": ["chat", "instruction_following", "code_generation"],
      "domain_specialization": ["general", "code"],
      "license": "MIT",
      "license_type": "permissive",
      "min_gpu_memory_gb": 8,
      "recommended_for": ["code_completion", "code_generation_detailed"],
      "approval_status": "approved",
      "notes": "4-bit quantized Phi-4"
    },
    {
      "model_id": "RedHatAI/phi-4-quantized.w8a8",
      "name": "Phi-4 W8A8",
      "provider": "Red Hat",
      "family": "phi",
      "size_parameters": "14B",
      "context_length": 16384,
      "supported_tasks": ["chat", "instruction_following", "code_generation"],
      "domain_specialization": ["general", "code"],
      "license": "MIT",
      "license_type": "permissive",
      "min_gpu_memory_gb": 10,
      "recommended_for": ["code_completion", "code_generation_detailed"],
      "approval_status": "approved",
      "notes": "8-bit quantized Phi-4"
    },
    {
      "model_id": "RedHatAI/Qwen2.5-7B-Instruct-FP8-dynamic",
      "name": "Qwen 2.5 7B Instruct FP8",
      "provider": "Red Hat",
      "family": "qwen",
      "size_parameters": "7B",
      "context_length": 131072,
      "supported_tasks": ["chat", "instruction_following", "code_generation", "multilingual"],
      "domain_specialization": ["general", "code", "multilingual"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 7,
      "recommended_for": ["chatbot_conversational", "code_generation_detailed", "translation"],
      "approval_status": "approved",
      "notes": "FP8 quantized Qwen 2.5"
    },
    {
      "model_id": "RedHatAI/Qwen2.5-7B-Instruct-quantized.w4a16",
      "name": "Qwen 2.5 7B Instruct W4A16",
      "provider": "Red Hat",
      "family": "qwen",
      "size_parameters": "7B",
      "context_length": 131072,
      "supported_tasks": ["chat", "instruction_following", "code_generation", "multilingual"],
      "domain_specialization": ["general", "code", "multilingual"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 4,
      "recommended_for": ["chatbot_conversational", "code_generation_detailed"],
      "approval_status": "approved",
      "notes": "4-bit quantized Qwen 2.5"
    },
    {
      "model_id": "RedHatAI/Qwen2.5-7B-Instruct-quantized.w8a8",
      "name": "Qwen 2.5 7B Instruct W8A8",
      "provider": "Red Hat",
      "family": "qwen",
      "size_parameters": "7B",
      "context_length": 131072,
      "supported_tasks": ["chat", "instruction_following", "code_generation", "multilingual"],
      "domain_specialization": ["general", "code", "multilingual"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 5,
      "recommended_for": ["chatbot_conversational", "code_generation_detailed"],
      "approval_status": "approved",
      "notes": "8-bit quantized Qwen 2.5"
    },
    {
      "model_id": "RedHatAI/Qwen3-8B-FP8-dynamic",
      "name": "Qwen3 8B FP8 Dynamic",
      "provider": "Red Hat",
      "family": "qwen",
      "size_parameters": "8B",
      "context_length": 131072,
      "supported_tasks": ["chat", "instruction_following", "code_generation", "multilingual"],
      "domain_specialization": ["general", "code", "multilingual"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 8,
      "recommended_for": ["chatbot_conversational", "code_generation_detailed", "translation"],
      "approval_status": "approved",
      "notes": "FP8 dynamic quantized Qwen3"
    },
    {
      "model_id": "RedHatAI/SmolLM3-3B-FP8-dynamic",
      "name": "SmolLM3 3B FP8",
      "provider": "Red Hat",
      "family": "smollm",
      "size_parameters": "3B",
      "context_length": 8192,
      "supported_tasks": ["chat", "instruction_following"],
      "domain_specialization": ["general"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 3,
      "recommended_for": ["chatbot_conversational", "code_completion"],
      "approval_status": "approved",
      "notes": "Ultra-compact model for edge deployment"
    },
    {
      "model_id": "RedHatAI/NVIDIA-Nemotron-Nano-9B-v2-FP8-dynamic",
      "name": "NVIDIA Nemotron Nano 9B FP8",
      "provider": "Red Hat",
      "family": "nemotron",
      "size_parameters": "9B",
      "context_length": 8192,
      "supported_tasks": ["chat", "instruction_following"],
      "domain_specialization": ["general"],
      "license": "NVIDIA Open Model License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 9,
      "recommended_for": ["chatbot_conversational", "content_generation"],
      "approval_status": "approved",
      "notes": "Compact NVIDIA Nemotron variant"
    },
    {
      "model_id": "RedHatAI/gemma-3n-E4B-it-FP8-dynamic",
      "name": "Gemma 3N E4B FP8",
      "provider": "Red Hat",
      "family": "gemma",
      "size_parameters": "4B",
      "context_length": 8192,
      "supported_tasks": ["chat", "instruction_following"],
      "domain_specialization": ["general"],
      "license": "Gemma License",
      "license_type": "permissive",
      "min_gpu_memory_gb": 4,
      "recommended_for": ["chatbot_conversational"],
      "approval_status": "approved",
      "notes": "FP8 quantized Gemma variant"
    },
    {
      "model_id": "RedHatAI/DeepSeek-R1-0528-quantized.w4a16",
      "name": "DeepSeek R1 W4A16",
      "provider": "Red Hat",
      "family": "deepseek",
      "size_parameters": "7B",
      "context_length": 16384,
      "supported_tasks": ["chat", "instruction_following", "reasoning"],
      "domain_specialization": ["general"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 4,
      "recommended_for": ["chatbot_conversational", "code_generation_detailed"],
      "approval_status": "approved",
      "notes": "4-bit quantized DeepSeek reasoning model"
    },
    {
      "model_id": "RedHatAI/Kimi-K2-Instruct-quantized.w4a16",
      "name": "Kimi K2 Instruct W4A16",
      "provider": "Red Hat",
      "family": "kimi",
      "size_parameters": "7B",
      "context_length": 16384,
      "supported_tasks": ["chat", "instruction_following"],
      "domain_specialization": ["general", "multilingual"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 4,
      "recommended_for": ["chatbot_conversational", "translation"],
      "approval_status": "approved",
      "notes": "4-bit quantized Kimi model with multilingual support"
    },
    {
      "model_id": "RedHatAI/Qwen3-Coder-480B-A35B-Instruct-FP8",
      "name": "Qwen3 Coder 480B FP8",
      "provider": "Red Hat",
      "family": "qwen",
      "size_parameters": "480B",
      "context_length": 32768,
      "supported_tasks": ["code_generation", "instruction_following"],
      "domain_specialization": ["code"],
      "license": "Apache 2.0",
      "license_type": "permissive",
      "min_gpu_memory_gb": 240,
      "recommended_for": ["code_generation_detailed"],
      "approval_status": "approved",
      "notes": "Massive code-specialized model"
    }
  ],
  "gpu_types": [
    {
      "gpu_type": "NVIDIA-L4",
      "memory_gb": 24,
      "compute_capability": "8.9",
      "typical_use_cases": ["inference"],
      "cost_per_hour_usd": 0.50,
      "availability": "high"
    },
    {
      "gpu_type": "NVIDIA-A10G",
      "memory_gb": 24,
      "compute_capability": "8.6",
      "typical_use_cases": ["inference"],
      "cost_per_hour_usd": 1.00,
      "availability": "high"
    },
    {
      "gpu_type": "NVIDIA-A100-40GB",
      "memory_gb": 40,
      "compute_capability": "8.0",
      "typical_use_cases": ["inference", "training"],
      "cost_per_hour_usd": 3.00,
      "availability": "medium"
    },
    {
      "gpu_type": "NVIDIA-A100-80GB",
      "memory_gb": 80,
      "compute_capability": "8.0",
      "typical_use_cases": ["inference", "training"],
      "cost_per_hour_usd": 4.50,
      "availability": "medium"
    },
    {
      "gpu_type": "H100",
      "memory_gb": 80,
      "compute_capability": "9.0",
      "typical_use_cases": ["inference", "training"],
      "cost_per_hour_usd": 8.00,
      "availability": "low"
    },
    {
      "gpu_type": "H200",
      "memory_gb": 141,
      "compute_capability": "9.0",
      "typical_use_cases": ["inference", "training"],
      "cost_per_hour_usd": 10.00,
      "availability": "low"
    }
  ]
}
