"""YAML Generation Module for KServe/vLLM Deployments.

This module generates production-ready Kubernetes YAML configurations for
LLM inference deployments using Jinja2 templates.
"""

import os
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional
from jinja2 import Environment, FileSystemLoader, Template

from ..context_intent.schema import DeploymentRecommendation

logger = logging.getLogger(__name__)


class DeploymentGenerator:
    """Generate deployment configurations from recommendations."""

    # GPU pricing (USD per hour) - representative cloud pricing
    GPU_PRICING = {
        "NVIDIA-L4": 0.50,
        "NVIDIA-A10G": 1.20,
        "NVIDIA-A100-40GB": 3.20,
        "NVIDIA-A100-80GB": 4.10,
        "NVIDIA-H100": 8.00,
    }

    # vLLM version to use
    VLLM_VERSION = "v0.6.2"

    def __init__(self, output_dir: Optional[str] = None):
        """
        Initialize the deployment generator.

        Args:
            output_dir: Directory to write generated YAML files (default: generated_configs/)
        """
        # Set up template environment
        template_dir = Path(__file__).parent / "templates"
        self.env = Environment(
            loader=FileSystemLoader(str(template_dir)),
            trim_blocks=True,
            lstrip_blocks=True
        )

        # Set output directory
        if output_dir:
            self.output_dir = Path(output_dir)
        else:
            # Default to generated_configs/ in project root
            project_root = Path(__file__).parent.parent.parent.parent
            self.output_dir = project_root / "generated_configs"

        # Create output directory if it doesn't exist
        self.output_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"DeploymentGenerator initialized with output_dir: {self.output_dir}")

    def generate_deployment_id(self, recommendation: DeploymentRecommendation) -> str:
        """
        Generate a unique deployment ID.

        Args:
            recommendation: Deployment recommendation

        Returns:
            Deployment ID (e.g., "chatbot-llama3-8b-20251003-143022")
        """
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        use_case = recommendation.intent.use_case.replace("_", "-")
        model_name = recommendation.model_id.split("/")[-1].lower().replace("_", "-")

        return f"{use_case}-{model_name}-{timestamp}"

    def _prepare_template_context(
        self,
        recommendation: DeploymentRecommendation,
        deployment_id: str,
        namespace: str = "default"
    ) -> Dict[str, Any]:
        """
        Prepare context dictionary for Jinja2 templates.

        Args:
            recommendation: Deployment recommendation
            deployment_id: Generated deployment ID
            namespace: Kubernetes namespace

        Returns:
            Context dictionary with all template variables
        """
        gpu_config = recommendation.gpu_config
        traffic = recommendation.traffic_profile
        slo = recommendation.slo_targets

        # Calculate GPU hourly rate
        gpu_hourly_rate = self.GPU_PRICING.get(gpu_config.gpu_type, 1.0)

        # Determine resource requests based on GPU type
        if "H100" in gpu_config.gpu_type or "A100" in gpu_config.gpu_type:
            cpu_request = "24"
            cpu_limit = "48"
            memory_request = "128Gi"
            memory_limit = "256Gi"
        elif "A10G" in gpu_config.gpu_type:
            cpu_request = "12"
            cpu_limit = "24"
            memory_request = "64Gi"
            memory_limit = "128Gi"
        else:  # L4 and others
            cpu_request = "8"
            cpu_limit = "16"
            memory_request = "32Gi"
            memory_limit = "64Gi"

        # Calculate autoscaling parameters
        min_replicas = max(1, gpu_config.replicas // 2)
        max_replicas = gpu_config.replicas * 2

        # Determine max model length based on use case
        max_model_len_map = {
            "chatbot": 4096,
            "customer_service": 4096,
            "code_generation": 8192,
            "summarization": 8192,
            "content_creation": 8192,
            "qa_retrieval": 4096,
            "batch_analytics": 16384,
        }
        max_model_len = max_model_len_map.get(recommendation.intent.use_case, 4096)

        # Calculate max_num_seqs based on expected QPS and latency
        # Rule of thumb: concurrent requests = QPS Ã— avg_latency_seconds
        avg_latency_sec = slo.e2e_p95_target_ms / 1000.0
        max_num_seqs = max(32, int(traffic.expected_qps * avg_latency_sec * 1.5))

        # Max batched tokens (vLLM parameter)
        max_num_batched_tokens = max_num_seqs * (traffic.prompt_tokens_mean + traffic.generation_tokens_mean)

        context = {
            # Deployment metadata
            "deployment_id": deployment_id,
            "namespace": namespace,
            "model_id": recommendation.model_id,
            "model_name": recommendation.model_name,
            "use_case": recommendation.intent.use_case,
            "reasoning": recommendation.reasoning,
            "generated_at": datetime.now().isoformat(),

            # GPU configuration
            "gpu_type": gpu_config.gpu_type,
            "gpu_count": gpu_config.gpu_count,
            "tensor_parallel": gpu_config.tensor_parallel,
            "gpus_per_replica": gpu_config.tensor_parallel,  # GPUs per pod

            # vLLM configuration
            "vllm_version": self.VLLM_VERSION,
            "dtype": "auto",  # Let vLLM auto-detect (float16, bfloat16, etc.)
            "gpu_memory_utilization": 0.9,  # Use 90% of GPU memory
            "max_model_len": max_model_len,
            "max_num_seqs": max_num_seqs,
            "max_num_batched_tokens": max_num_batched_tokens,
            "max_batch_size": max_num_seqs,
            "enable_prefix_caching": True,  # Enable KV cache optimization

            # Autoscaling
            "min_replicas": min_replicas,
            "max_replicas": max_replicas,
            "autoscaling_metric": "inference_requests_concurrency",
            "autoscaling_target": "10",  # Target 10 concurrent requests per pod
            "queue_depth_threshold": "20",

            # Resource requests
            "cpu_request": cpu_request,
            "cpu_limit": cpu_limit,
            "memory_request": memory_request,
            "memory_limit": memory_limit,

            # SLO targets
            "ttft_target": slo.ttft_p90_target_ms,
            "tpot_target": slo.tpot_p90_target_ms,
            "e2e_target": slo.e2e_p95_target_ms,
            "target_qps": traffic.expected_qps,

            # Traffic profile
            "expected_qps": traffic.expected_qps,
            "prompt_tokens_mean": traffic.prompt_tokens_mean,
            "generation_tokens_mean": traffic.generation_tokens_mean,

            # Cost estimation
            "cost_per_hour": recommendation.cost_per_hour_usd,
            "cost_per_month": recommendation.cost_per_month_usd,
            "gpu_hourly_rate": gpu_hourly_rate,

            # Intent metadata
            "user_count": recommendation.intent.user_count,
            "latency_requirement": recommendation.intent.latency_requirement,
            "budget_constraint": recommendation.intent.budget_constraint,
        }

        return context

    def generate_all(
        self,
        recommendation: DeploymentRecommendation,
        namespace: str = "default"
    ) -> Dict[str, str]:
        """
        Generate all deployment YAML files.

        Args:
            recommendation: Deployment recommendation
            namespace: Kubernetes namespace

        Returns:
            Dictionary mapping config type to file path
        """
        deployment_id = self.generate_deployment_id(recommendation)
        context = self._prepare_template_context(recommendation, deployment_id, namespace)

        generated_files = {}

        # Generate each config file
        configs = [
            ("kserve-inferenceservice.yaml.j2", f"{deployment_id}-inferenceservice.yaml"),
            ("vllm-config.yaml.j2", f"{deployment_id}-vllm-config.yaml"),
            ("autoscaling.yaml.j2", f"{deployment_id}-autoscaling.yaml"),
            ("servicemonitor.yaml.j2", f"{deployment_id}-servicemonitor.yaml"),
        ]

        for template_name, output_filename in configs:
            try:
                template = self.env.get_template(template_name)
                rendered = template.render(**context)

                output_path = self.output_dir / output_filename
                with open(output_path, "w") as f:
                    f.write(rendered)

                config_type = output_filename.split("-", 1)[-1].replace(".yaml", "")
                generated_files[config_type] = str(output_path)

                logger.info(f"Generated {config_type}: {output_path}")

            except Exception as e:
                logger.error(f"Failed to generate {template_name}: {e}")
                raise

        # Store deployment metadata
        metadata = {
            "deployment_id": deployment_id,
            "namespace": namespace,
            "generated_at": context["generated_at"],
            "files": generated_files,
            "recommendation": recommendation.model_dump(),
        }

        return {
            "deployment_id": deployment_id,
            "namespace": namespace,
            "files": generated_files,
            "metadata": metadata,
        }

    def generate_kserve_yaml(
        self,
        recommendation: DeploymentRecommendation,
        deployment_id: Optional[str] = None,
        namespace: str = "default"
    ) -> str:
        """
        Generate only the KServe InferenceService YAML.

        Args:
            recommendation: Deployment recommendation
            deployment_id: Optional deployment ID (auto-generated if not provided)
            namespace: Kubernetes namespace

        Returns:
            Path to generated YAML file
        """
        if not deployment_id:
            deployment_id = self.generate_deployment_id(recommendation)

        context = self._prepare_template_context(recommendation, deployment_id, namespace)
        template = self.env.get_template("kserve-inferenceservice.yaml.j2")
        rendered = template.render(**context)

        output_path = self.output_dir / f"{deployment_id}-inferenceservice.yaml"
        with open(output_path, "w") as f:
            f.write(rendered)

        logger.info(f"Generated KServe YAML: {output_path}")
        return str(output_path)
